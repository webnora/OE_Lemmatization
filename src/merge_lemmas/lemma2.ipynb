{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "import itertools\n",
    "from bs4 import BeautifulSoup\n",
    "path_data = '../../data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(s):\n",
    "    for v in [('ġ', 'g'), ('ƿ', 'w'), ('ċ', 'с')]:\n",
    "        s = s.replace(v[0], v[1]) \n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nesan\t''unattested''\n",
    "def filter_forms(arr):\n",
    "    return sorted(set(v for v in arr if v not in ['tō', \"''unattested''\", '']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'file': 'oe.lemmas',\n",
       " 'lemma': 'macian',\n",
       " 'forms': ['gemacod,macod',\n",
       "  'maca',\n",
       "  'macast',\n",
       "  'macaþ',\n",
       "  'macian',\n",
       "  'maciaþ,macigaþ',\n",
       "  'macie,macige',\n",
       "  'macien,macigen',\n",
       "  'maciende,macigende',\n",
       "  'macienne',\n",
       "  'macode',\n",
       "  'macoden',\n",
       "  'macodest',\n",
       "  'macodon']}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def load_oe(file = 'oe.lemmas'):\n",
    "    def make(s):\n",
    "        a = s.split()\n",
    "        return {\n",
    "            \"file\": file,\n",
    "            \"lemma\": a[0],\n",
    "            \"forms\": filter_forms(a[1:])\n",
    "        }\n",
    "    with open(path_data + file, 'r') as f:\n",
    "        arr = normalize(f.read()).split('\\n')\n",
    "        return [make(s) for s in arr if s not in ['', '&\\t&']]\n",
    "load_oe()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'file': 'ang.txt',\n",
       " 'lemma': 'feohtan',\n",
       " 'forms': ['feaht',\n",
       "  'feoht',\n",
       "  'feohtan',\n",
       "  'feohtaþ',\n",
       "  'feohte',\n",
       "  'feohten',\n",
       "  'feohtende',\n",
       "  'fieht',\n",
       "  'fiehtst',\n",
       "  'fohten',\n",
       "  'fuhte',\n",
       "  'fuhten',\n",
       "  'fuhton']}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def load_dict(file = 'ang.txt'):\n",
    "    def make(w):\n",
    "        return {\n",
    "            \"file\": file,\n",
    "            \"lemma\": w.split()[0],\n",
    "            \"forms\": filter_forms([l.split()[1] for l in w.split('\\n')])\n",
    "        }\n",
    "    with open(path_data + file, 'r') as f:\n",
    "        arr = normalize(f.read()).split('\\n\\n')\n",
    "        return [make(w) for w in arr if len(w) != 0]\n",
    "load_dict()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../../data/datasets/node.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-a75888ef27ba>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mmake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mitertools\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mislice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlimit\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m_filter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m6\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;31m# [v[\"forms\"] for v in load_node()]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mload_node\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-4-a75888ef27ba>\u001b[0m in \u001b[0;36mload_node\u001b[0;34m(limit, file)\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;31m#             \"forms\": pars_grammar(w[7])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         }\n\u001b[0;32m---> 21\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath_data\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m         \u001b[0mgen\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcsv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdelimiter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m';'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mmake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mitertools\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mislice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlimit\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m_filter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m6\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../../data/datasets/node.csv'"
     ]
    }
   ],
   "source": [
    "def load_node(limit = 100000000, file = 'datasets/node.csv'):\n",
    "    def pars_grammar(s):\n",
    "        gr = BeautifulSoup(s).find('grammar')\n",
    "        if not gr: return []\n",
    "        group = ''.join(gr.findAll(text=True, recursive=False)).split(';')\n",
    "        res = [item.strip() for sublist in (g.split(',') for g in group) for item in sublist]\n",
    "#         print(gr.prettify())\n",
    "#         print('====>', res)\n",
    "        return res\n",
    "    def _filter(w):\n",
    "#         return True\n",
    "        return w.isupper() and len(w) > 1\n",
    "    def make(w):\n",
    "#         print(w[6])\n",
    "        return {\n",
    "            \"file\": file,\n",
    "            \"lemma\": w[6].lower(),\n",
    "            \"forms\": filter_forms(pars_grammar(w[7]))\n",
    "#             \"forms\": pars_grammar(w[7])\n",
    "        }\n",
    "    with open(path_data + file, 'r') as f:\n",
    "        gen = csv.reader(f, delimiter=';')\n",
    "        return [make(w) for w in itertools.islice(gen, limit) if _filter(w[6])]\n",
    "# [v[\"forms\"] for v in load_node()]\n",
    "len(load_node(1000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'file': 'ang.txt',\n",
       " 'lemma': 'feohtan',\n",
       " 'forms': ['feaht',\n",
       "  'feoht',\n",
       "  'feohtan',\n",
       "  'feohtaþ',\n",
       "  'feohte',\n",
       "  'feohten',\n",
       "  'feohtende',\n",
       "  'fieht',\n",
       "  'fiehtst',\n",
       "  'fohten',\n",
       "  'fuhte',\n",
       "  'fuhten',\n",
       "  'fuhton']}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def merge_array(base, add):\n",
    "    def to_set(arr):\n",
    "        return set(e[\"lemma\"] for e in arr)\n",
    "    diff = to_set(add).difference(to_set(base))\n",
    "    diff_arr = [v for v in add if v[\"lemma\"] in diff]\n",
    "    return base + diff_arr\n",
    "merge_array(load_dict(), load_oe())[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Bryttas',\n",
       " 'Crist',\n",
       " 'Dene',\n",
       " 'Engle',\n",
       " 'Englisc',\n",
       " 'Nazarenisc',\n",
       " 'Seaxisc',\n",
       " 'Wendlas',\n",
       " 'a',\n",
       " 'abarimathia']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def sort_dict(arr):\n",
    "    def get_key(v):\n",
    "        return v[\"lemma\"]\n",
    "    arr.sort(key=get_key)\n",
    "    return arr\n",
    "[v['lemma'] for v in sort_dict(merge_array(load_dict(), load_oe()))[:10]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'file': 'ang.txt',\n",
       " 'lemma': 'Bryttas',\n",
       " 'forms': ['Brytta', 'Bryttas', 'Bryttum']}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_all():\n",
    "    return sort_dict(merge_array(load_dict(), load_oe()))\n",
    "get_all()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_cltk(arr, file='out.txt'):\n",
    "    with open(file, 'w') as f:\n",
    "        for v in arr:\n",
    "            for w in v[\"forms\"]:\n",
    "                s = f'{v[\"lemma\"]}\\t{w}\\n'\n",
    "                f.write(s)\n",
    "            f.write(\"\\n\")\n",
    "save_cltk(get_all())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_dict(arr, file='out_dict.txt'):\n",
    "    with open(file, 'w') as f:\n",
    "        for v in arr:\n",
    "            f.write(f'{v[\"lemma\"]}\\t{\"|\".join(v[\"forms\"])}\\n')\n",
    "save_dict(get_all())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_dict((w for w in merge_array(get_all(), load_node()) if w['file'] == 'datasets/node.csv'), file='out_node.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6367"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(get_all())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
